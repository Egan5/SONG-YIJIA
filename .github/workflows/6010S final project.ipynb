{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preprocessing\n",
    "#Loading data\n",
    "def get_data(path_neg, path_pos):\n",
    "    neg_data = []\n",
    "    pos_data = []\n",
    " \n",
    "    files_neg = glob.glob(path_neg)\n",
    "    files_pos = glob.glob(path_pos)\n",
    " \n",
    "    for neg in files_neg:\n",
    "        with open(neg, 'r', encoding='utf-8') as neg_f:\n",
    "            neg_data.append(neg_f.readline())\n",
    " \n",
    "    for pos in files_pos:\n",
    "        with open(pos, 'r', encoding='utf-8') as pos_f:\n",
    "            pos_data.append(pos_f.readline())\n",
    " \n",
    "    neg_label = np.zeros(len(neg_data)).tolist()\n",
    "    pos_label = np.ones(len(pos_data)).tolist()\n",
    " \n",
    "    corpus = neg_data + pos_data\n",
    "    labels = neg_label + pos_label\n",
    " \n",
    "    return corpus, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\songy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize and preprocess data\n",
    "def normalize(corpus):\n",
    "    normalized_corpus = []\n",
    " \n",
    "    for text in corpus:\n",
    "        # Convert to lowercase\n",
    "        text = text.lower().strip()\n",
    " \n",
    "        # Remove symbol\n",
    "        text = re.sub(r\"<br />\", r\" \", text)\n",
    "        text = re.sub(' +', ' ', text)\n",
    "        text = re.sub(r'(\\W)(?=\\1)', '', text)\n",
    "        text = re.sub(r\"([.!?])\", r\" \\1\", text)\n",
    "        text = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", text)\n",
    " \n",
    "        # Separate words and remove punctuation marks\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        tokens = tokenizer.tokenize(text)\n",
    " \n",
    "        # Remove stop words\n",
    "        stopword = stopwords.words('english')\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword]\n",
    " \n",
    "        # Regroup\n",
    "        filtered_text = ' '.join(filtered_tokens)\n",
    "        normalized_corpus.append(filtered_text)\n",
    " \n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract features\n",
    "#Bag of Words model\n",
    "def bow_extractor(corpus, ngram_range=(1, 1)):\n",
    "    vectorizer = CountVectorizer(min_df=1, ngram_range=ngram_range)\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, features\n",
    " \n",
    "#TF-IDF model\n",
    "def tfidf_extractor(corpus, ngram_range=(1, 1)):\n",
    "    vectorizer = TfidfVectorizer(min_df=1, norm='l2', smooth_idf=True, use_idf=True, ngram_range=ngram_range)\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "svm = SGDClassifier(loss='hinge', max_iter=100)\n",
    "lr = LogisticRegression(solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training classifier\n",
    "def train_predict_evaluate_model(classifier,\n",
    "                                 train_features, train_labels,\n",
    "                                 test_features, test_labels):\n",
    "    # Training\n",
    "    classifier.fit(train_features, train_labels)\n",
    "    # Predict results on the test set\n",
    "    predictions = classifier.predict(test_features)\n",
    " \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model evaluation\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    " \n",
    "#Evaluation index\n",
    "def get_metrics(true_labels, predicted_labels):\n",
    "    print('Accuracy:', np.round(\n",
    "        metrics.accuracy_score(true_labels,\n",
    "                               predicted_labels),\n",
    "        2))\n",
    "    print('Precision:', np.round(\n",
    "        metrics.precision_score(true_labels,\n",
    "                                predicted_labels,\n",
    "                                average='weighted'),\n",
    "        2))\n",
    "    print('Recall rate:', np.round(\n",
    "        metrics.recall_score(true_labels,\n",
    "                             predicted_labels,\n",
    "                             average='weighted'),\n",
    "        2))\n",
    "    print('F-measure:', np.round(\n",
    "        metrics.f1_score(true_labels,\n",
    "                         predicted_labels,\n",
    "                         average='weighted'),\n",
    "        2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression model based on features of Bag of Words model\n",
      "Accuracy: 0.86\n",
      "Precision: 0.86\n",
      "Recall rate: 0.86\n",
      "F-measure: 0.86\n",
      "SVM model based on Bag of features of Bag of Words model\n",
      "Accuracy: 0.85\n",
      "Precision: 0.85\n",
      "Recall rate: 0.85\n",
      "F-measure: 0.85\n",
      "Logistic regression model based on features of tfidf model\n",
      "Accuracy: 0.88\n",
      "Precision: 0.88\n",
      "Recall rate: 0.88\n",
      "F-measure: 0.88\n",
      "SVM model based on Bag of features of tfidf model\n",
      "Accuracy: 0.88\n",
      "Precision: 0.88\n",
      "Recall rate: 0.88\n",
      "F-measure: 0.88\n"
     ]
    }
   ],
   "source": [
    "#Main function \n",
    "if __name__ == \"__main__\":\n",
    "    train_corpus, train_labels = get_data('C:/Users/songy/Documents/2020spring data/aclImdb/train/neg/*.txt', 'C:/Users/songy/Documents/2020spring data/aclImdb/train/pos/*.txt')\n",
    "    test_corpus, test_labels = get_data('C:/Users/songy/Documents/2020spring data/aclImdb/test/neg/*.txt', 'C:/Users/songy/Documents/2020spring data/aclImdb/test/pos/*.txt')\n",
    " \n",
    "    norm_train_corpus = normalize(train_corpus)\n",
    "    norm_test_corpus = normalize(test_corpus)\n",
    " \n",
    "    # Bag of Words model features\n",
    "    bow_vectorizer, bow_train_features = bow_extractor(norm_train_corpus)\n",
    "    bow_test_features = bow_vectorizer.transform(norm_test_corpus)\n",
    " \n",
    "    # tfidf model features\n",
    "    tfidf_vectorizer, tfidf_train_features = tfidf_extractor(norm_train_corpus)\n",
    "    tfidf_test_features = tfidf_vectorizer.transform(norm_test_corpus)\n",
    " \n",
    "    # Import classifier\n",
    "    svm = SGDClassifier(loss='hinge', max_iter=100)\n",
    "    lr = LogisticRegression(solver='liblinear')\n",
    " \n",
    "    # Logistic regression model based on features of Bag of Words model\n",
    "    print(\"Logistic regression model based on features of Bag of Words model\")\n",
    "    lr_bow_predictions = train_predict_evaluate_model(classifier=lr,\n",
    "                                                      train_features=bow_train_features,\n",
    "                                                      train_labels=train_labels,\n",
    "                                                      test_features=bow_test_features,\n",
    "                                                      test_labels=test_labels)\n",
    "    results1 = get_metrics(test_labels,lr_bow_predictions)\n",
    "    results1\n",
    " \n",
    "    # SVM model based on Bag of features of Bag of Words model\n",
    "    print(\"SVM model based on Bag of features of Bag of Words model\")\n",
    "    svm_bow_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                                       train_features=bow_train_features,\n",
    "                                                       train_labels=train_labels,\n",
    "                                                       test_features=bow_test_features,\n",
    "                                                       test_labels=test_labels)\n",
    "    results2 = get_metrics(test_labels,svm_bow_predictions)\n",
    "    results2\n",
    "    \n",
    "    # Logistic regression model based on features of tfidf model\n",
    "    print(\"Logistic regression model based on features of tfidf model\")\n",
    "    lr_tfidf_predictions = train_predict_evaluate_model(classifier=lr,\n",
    "                                                        train_features=tfidf_train_features,\n",
    "                                                        train_labels=train_labels,\n",
    "                                                        test_features=tfidf_test_features,\n",
    "                                                        test_labels=test_labels)\n",
    "    results3 = get_metrics(test_labels,lr_tfidf_predictions)\n",
    "    results3 \n",
    "    # SVM model based on Bag of features of tfidf model\n",
    "    print(\"SVM model based on Bag of features of tfidf model\")\n",
    "    svm_tfidf_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                                         train_features=tfidf_train_features,\n",
    "                                                         train_labels=train_labels,\n",
    "                                                         test_features=tfidf_test_features,\n",
    "                                                         test_labels=test_labels)\n",
    "    results4 = get_metrics(test_labels,svm_tfidf_predictions)\n",
    "    results4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
